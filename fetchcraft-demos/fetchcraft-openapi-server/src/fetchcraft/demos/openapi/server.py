"""
RAG Agent OpenAPI Tool Integration Demo

This demo provides a clean OpenAPI interface for the RAG agent that can be
integrated as a tool by other AI agents or systems.

Features:
- Simple query endpoint for RAG operations
- OpenAPI schema auto-generated by FastAPI
- Tool definition endpoint for easy integration
- Health check and status endpoints
- Proper error handling and validation

Usage:
    python -m demo.openapi_server.rag_tool_api

    # Test with curl:
    curl -X POST http://localhost:8002/query \
      -H "Content-Type: application/json" \
      -d '{"question": "What is this about?"}'

    # Get tool definition for integration:
    curl http://localhost:8002/tool-definition
"""

import os
import time
from contextlib import asynccontextmanager
from pathlib import Path
from typing import List, Optional

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic_ai import Tool
from qdrant_client import QdrantClient

from fetchcraft.agents import RetrieverTool, PydanticAgent
from fetchcraft.document_store import MongoDBDocumentStore
from fetchcraft.embeddings import OpenAIEmbeddings
from fetchcraft.index.vector_index import VectorIndex
from fetchcraft.node import SymNode
from fetchcraft.node_parser import HierarchicalNodeParser
from fetchcraft.demos.openapi import QueryRequest, QueryResponse, HealthResponse, Citation
from fetchcraft.parsing.filesystem import FilesystemDocumentParser
from fetchcraft.vector_store import QdrantVectorStore

load_dotenv()

# ============================================================================
# Configuration
# ============================================================================

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "fetchcraft_chatbot")
DOCUMENTS_PATH = Path(os.getenv("DOCUMENTS_PATH", "Documents"))

# Embeddings configuration
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3")
EMBEDDING_API_KEY = os.getenv("OPENAI_API_KEY", "sk-321")
EMBEDDING_BASE_URL = os.getenv("OPENAI_BASE_URL", os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"))
INDEX_ID = os.getenv("INDEX_ID", "docs-index")

# LLM configuration
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4-turbo")

# Chunking configuration
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "8192"))
CHILD_CHUNKS = [int(chunk_size) for chunk_size in os.getenv("CHILD_CHUNKS", "4096,1024").split(",")]
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))

# Hybrid search configuration
ENABLE_HYBRID = bool(os.getenv("ENABLE_HYBRID", True))
FUSION_METHOD = os.getenv("FUSION_METHOD", "rrf")

# Server configuration
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8001"))

OUTPUT_HTML=os.getenv("OUTPUT_HTML", False)

# ============================================================================
# Request/Response Models
# ============================================================================


# ============================================================================
# Global State
# ============================================================================

class AppState:
    """Global application state."""
    agent: Optional[PydanticAgent] = None
    vector_index: Optional[VectorIndex] = None
    output_html: bool = OUTPUT_HTML
    initialized: bool = False


app_state = AppState()


# ============================================================================
# RAG System Setup
# ============================================================================


async def load_and_index_documents(
    vector_index: VectorIndex,
    doc_store: MongoDBDocumentStore,
    documents_path: Path,
    chunk_size: int = 8192,
    child_sizes: List[int] = None,
    overlap: int = 200,
) -> int:
    """Load documents from a directory and index them."""
    print(f"\nðŸ“‚ Loading documents from: {documents_path}")

    if not documents_path.exists():
        print(f"âš ï¸  Documents path does not exist: {documents_path}")
        return 0

    if child_sizes is None:
        child_sizes = [4096, 1024]

    # Load documents from filesystem
    source = FilesystemDocumentParser.from_directory(
        directory=documents_path,
        pattern="*",
        recursive=True
    )

    documents = []
    async for doc in source.get_documents():
        documents.append(doc)
        await doc_store.add_document(doc)

    if not documents:
        print("âš ï¸  No files found in the specified directory!")
        return 0

    print(f"  âœ“ Loaded {len(documents)} documents")

    # Parse documents into nodes
    parser = HierarchicalNodeParser(
        chunk_size=chunk_size,
        overlap=overlap,
        child_sizes=child_sizes,
        child_overlap=50
    )

    all_nodes = parser.get_nodes(documents)
    all_chunks = [n for n in all_nodes if isinstance(n, SymNode)]
    print(f"  âœ“ Created {len(all_chunks)} chunks for indexing")

    # Index all chunks
    await vector_index.add_nodes(all_chunks, show_progress=True)

    print(f"âœ… Successfully indexed {len(all_chunks)} chunks!")
    return len(all_chunks)


async def setup_rag_system():
    """Set up the RAG system."""
    print("=" * 70)
    print("ðŸš€ RAG Tool API - OpenAPI Integration")
    print("=" * 70)

    # Initialize embeddings
    print("\n1ï¸âƒ£  Initializing embeddings...")
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        api_key=EMBEDDING_API_KEY,
        base_url=EMBEDDING_BASE_URL
    )

    print(f"\n2ï¸âƒ£  Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}...")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    needs_indexing = COLLECTION_NAME not in [collection.name for collection in client.get_collections().collections]

    # Create vector store
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embeddings=embeddings,
        distance="Cosine",
        enable_hybrid=ENABLE_HYBRID,
        fusion_method=FUSION_METHOD
    )

    doc_store = MongoDBDocumentStore(
        database_name="fetchcraft",
        collection_name=COLLECTION_NAME,
    )

    vector_index = VectorIndex(
        vector_store=vector_store,
        doc_store=doc_store,
        index_id=INDEX_ID
    )
    print(f"   âœ“ Vector index created")

    # Index documents if needed
    if needs_indexing:
        print(f"\n4ï¸âƒ£  Indexing documents...")
        await load_and_index_documents(
            vector_index=vector_index,
            doc_store=doc_store,
            documents_path=DOCUMENTS_PATH,
            chunk_size=CHUNK_SIZE,
            child_sizes=CHILD_CHUNKS,
            overlap=CHUNK_OVERLAP
        )
    else:
        print(f"\n4ï¸âƒ£  Skipping document indexing (collection already exists)")

    # Create retriever
    print(f"\n5ï¸âƒ£  Creating retriever...")
    retriever = vector_index.as_retriever(top_k=3, resolve_parents=True)
    print(f"   âœ“ Retriever created")

    # Create agent
    print(f"\n6ï¸âƒ£  Creating RAG agent...")
    retriever_tool = RetrieverTool.from_retriever(retriever)
    tool_func = retriever_tool.get_tool_function()
    tools = [Tool(tool_func, takes_ctx=True, max_retries=3)]

    agent = PydanticAgent.create(
        model=LLM_MODEL,
        tools=tools,
        retries=3
    )
    print(f"   âœ“ Agent created with model: {LLM_MODEL}")

    print("\n" + "=" * 70)
    print("âœ… RAG System Ready!")
    print("=" * 70)

    return agent, vector_index


# ============================================================================
# FastAPI Lifespan
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize the RAG system on startup."""
    try:

        print("\n Configuring RAG system...")
        agent, vector_index = await setup_rag_system()
        app_state.agent = agent
        app_state.vector_index = vector_index
        app_state.initialized = True
        print(f"\nâœ… Server ready at http://{HOST}:{PORT}")
        print(f"   API docs: http://{HOST}:{PORT}/docs")
        print(f"   OpenAPI schema: http://{HOST}:{PORT}/openapi.json")
        print("=" * 70 + "\n")
        yield
    except Exception as e:
        print(f"\nâŒ Startup Error: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print("\nðŸ‘‹ Shutting down server...")


# ============================================================================
# FastAPI App
# ============================================================================

app = FastAPI(
    title="RAG Tool API",
    description=(
        "This api can be used to search for information."
    ),
    version="1.0.0",
    lifespan=lifespan,
    contact={
        "name": "RAG Tool API",
        "url": "https://github.com/vaaale/fetchcraft",
    },
    license_info={
        "name": "MIT",
    },
)

# Add CORS middleware to handle OPTIONS requests and cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (configure for production)
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods including OPTIONS
    allow_headers=["*"],  # Allow all headers
)


# ============================================================================
# API Endpoints
# ============================================================================

@app.get("/", tags=["General"])
async def root():
    """
    Root endpoint with API information.

    Returns basic information about the API and available endpoints.
    """
    return {
        "name": "RAG Tool API",
        "version": "1.0.0",
        "description": "Search for information.",
        "endpoints": {
            "query": "/query",
            "health": "/health",
            "tool_definition": "/tool-definition",
            "docs": "/docs",
            "openapi": "/openapi.json"
        }
    }


@app.get("/health", response_model=HealthResponse, tags=["General"])
async def health():
    """
    Health check endpoint.

    Returns the current status of the service and configuration information.
    """
    return HealthResponse(
        status="healthy" if app_state.initialized else "initializing",
        version="1.0.0",
        model=LLM_MODEL,
        collection=COLLECTION_NAME,
        hybrid_search_enabled=ENABLE_HYBRID
    )


@app.post("/query", response_model=QueryResponse, tags=["RAG"])
async def query(request: QueryRequest):
    """
    Query the RAG agent with a question.

    This endpoint retrieves relevant documents from the vector database and uses
    an LLM to generate a comprehensive answer based on the retrieved context.

    Args:
        request: Query request containing the question and optional parameters

    Returns:
        QueryResponse with the answer and optional citations

    Raises:
        HTTPException: If the service is not initialized or an error occurs
    """
    if not app_state.initialized or not app_state.agent:
        raise HTTPException(
            status_code=503,
            detail="RAG system not initialized. Please try again in a moment."
        )

    start_time = time.time()

    try:
        # Query the agent
        response = await app_state.agent.query(request.question)

        # Extract answer
        answer = response.response.content

        # Extract citations if requested
        citations = None
        if request.include_citations and response.citations:
            citations = []
            for citation in response.citations[:request.top_k]:
                citations.append(Citation(
                    source=citation.node.metadata.get("parsing", "Unknown"),
                    filename=citation.node.metadata.get("filename", "N/A"),
                    score=float(citation.node.score) if hasattr(citation.node, "score") else 0.0,
                    text_preview=(
                        citation.node.text[:200] + "..."
                        if len(citation.node.text) > 200
                        else citation.node.text
                    )
                ))

        processing_time = (time.time() - start_time) * 1000  # Convert to ms

        if app_state.output_html:
            from fetchcraft.demos.openapi.jinja_renderer import render_response
            from starlette.responses import HTMLResponse

            html_output = render_response(
                heading="Answer generated by Wingman ðŸ¤–",  # <-- new optional heading
                answer=answer,
                processing_time=processing_time,
                llm_model=LLM_MODEL,
                citations=citations,
                answer_is_html=False,
                files_base_url="http://wingman.akhbar.home/files",
            )

            return HTMLResponse(
                content=html_output,
                headers={"Content-Disposition": "inline"}
            )
        else:
            return QueryResponse(
                answer=answer,
                citations=citations,
                processing_time_ms=round(processing_time, 2),
                model=LLM_MODEL
            )

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Error processing query: {str(e)}"
        )


def main():
    """Run the FastAPI server."""
    import uvicorn

    print("\n" + "=" * 70)
    print("ðŸš€ Starting RAG Tool API Server")
    print("=" * 70)
    print(f"\nConfiguration:")
    print(f"  â€¢ Host: {HOST}")
    print(f"  â€¢ Port: {PORT}")
    print(f"  â€¢ Model: {LLM_MODEL}")
    print(f"  â€¢ Collection: {COLLECTION_NAME}")
    print(f"  â€¢ Hybrid Search: {ENABLE_HYBRID}")
    print(f"  â€¢ Output HTML: {OUTPUT_HTML}")
    print("=" * 70 + "\n")

    uvicorn.run(
        "fetchcraft.demos.openapi.server:app",
        host=HOST,
        port=PORT,  # 8001
        reload=True,
        log_level="debug"
    )


if __name__ == "__main__":
    main()
