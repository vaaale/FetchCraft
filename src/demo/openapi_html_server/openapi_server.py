"""
RAG Agent OpenAPI Tool Integration Demo

This demo provides a clean OpenAPI interface for the RAG agent that can be
integrated as a tool by other AI agents or systems.

Features:
- Simple query endpoint for RAG operations
- OpenAPI schema auto-generated by FastAPI
- Tool definition endpoint for easy integration
- Health check and status endpoints
- Proper error handling and validation

Usage:
    python -m demo.openapi_server.rag_tool_api

    # Test with curl:
    curl -X POST http://localhost:8002/query \
      -H "Content-Type: application/json" \
      -d '{"question": "What is this about?"}'

    # Get tool definition for integration:
    curl http://localhost:8002/tool-definition
"""

import os
import time
from contextlib import asynccontextmanager
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from pydantic_ai import Tool
from qdrant_client import QdrantClient
from starlette.responses import HTMLResponse

from demo.openapi_html_server.jinja_renderer import render_response
# from demo.openai_api_demo.render_response import render_response
from fetchcraft.agents import RetrieverTool, PydanticAgent
from fetchcraft.embeddings import OpenAIEmbeddings
from fetchcraft.index.vector_index import VectorIndex
from fetchcraft.node import SymNode
from fetchcraft.node_parser import HierarchicalNodeParser
from fetchcraft.source import FilesystemDocumentSource
from fetchcraft.vector_store import QdrantVectorStore

# ============================================================================
# Configuration
# ============================================================================

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "fetchcraft_chatbot")
DOCUMENTS_PATH = Path(os.getenv("DOCUMENTS_PATH", "Documents"))

# Embeddings configuration
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3")
EMBEDDING_API_KEY = os.getenv("OPENAI_API_KEY", "sk-321")
EMBEDDING_BASE_URL = os.getenv("OPENAI_BASE_URL", None)
INDEX_ID = "docs-index"

# LLM configuration
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4-turbo")
LLM_API_KEY = os.getenv("OPENAI_API_KEY", "sk-123")

# Chunking configuration
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "8192"))
CHILD_SIZES = [4096, 1024]
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))

# Hybrid search configuration
ENABLE_HYBRID = os.getenv("ENABLE_HYBRID", "true").lower() == "true"
FUSION_METHOD = os.getenv("FUSION_METHOD", "rrf")

# Server configuration
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8001"))


# ============================================================================
# Request/Response Models
# ============================================================================

class QueryRequest(BaseModel):
    """Request model for querying the RAG agent."""
    question: str = Field(
        description="The question to ask the RAG agent",
        examples=["What is the main topic discussed in the documents?"]
    )
    top_k: Optional[int] = Field(
        default=3,
        description="Number of documents to retrieve",
        ge=1,
        le=10
    )
    include_citations: Optional[bool] = Field(
        default=True,
        description="Whether to include source citations in the response"
    )


class Citation(BaseModel):
    """Citation information from retrieved documents."""
    source: str = Field(description="Source document identifier")
    filename: str = Field(description="Name of the source file")
    score: float = Field(description="Relevance score of this citation")
    text_preview: str = Field(description="Preview of the cited text")


class QueryResponse(BaseModel):
    """Response model for RAG queries."""
    answer: str = Field(description="The agent's answer to the question")
    citations: Optional[List[Citation]] = Field(
        default=None,
        description="List of citations used to generate the answer"
    )
    processing_time_ms: float = Field(description="Time taken to process the query in milliseconds")
    model: str = Field(description="LLM model used for generation")


class HealthResponse(BaseModel):
    """Health check response."""
    status: str = Field(description="Service status")
    version: str = Field(description="API version")
    model: str = Field(description="LLM model in use")
    collection: str = Field(description="Vector database collection name")
    hybrid_search_enabled: bool = Field(description="Whether hybrid search is enabled")


class ToolDefinition(BaseModel):
    """Tool definition for integration with AI agents."""
    type: str = Field(default="function", description="Tool type")
    function: Dict[str, Any] = Field(description="Function definition")


# ============================================================================
# Global State
# ============================================================================

class AppState:
    """Global application state."""
    agent: Optional[PydanticAgent] = None
    vector_index: Optional[VectorIndex] = None
    initialized: bool = False


app_state = AppState()


# ============================================================================
# RAG System Setup
# ============================================================================

def collection_exists(client: QdrantClient, collection_name: str) -> bool:
    """Check if a collection exists in Qdrant."""
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]
    return collection_name in collection_names


async def load_and_index_documents(
    vector_index: VectorIndex,
    documents_path: Path,
    chunk_size: int = 8192,
    child_sizes: List[int] = None,
    overlap: int = 200,
) -> int:
    """Load documents from a directory and index them."""
    print(f"\nðŸ“‚ Loading documents from: {documents_path}")

    if not documents_path.exists():
        print(f"âš ï¸  Documents path does not exist: {documents_path}")
        return 0

    if child_sizes is None:
        child_sizes = [4096, 1024]

    # Load documents from filesystem
    source = FilesystemDocumentSource.from_directory(
        directory=documents_path,
        pattern="*",
        recursive=True
    )

    documents = []
    async for doc in source.get_documents():
        documents.append(doc)

    if not documents:
        print("âš ï¸  No files found in the specified directory!")
        return 0

    print(f"  âœ“ Loaded {len(documents)} documents")

    # Parse documents into nodes
    parser = HierarchicalNodeParser(
        chunk_size=chunk_size,
        overlap=overlap,
        child_sizes=child_sizes,
        child_overlap=50
    )

    all_nodes = parser.get_nodes(documents)
    all_chunks = [n for n in all_nodes if isinstance(n, SymNode)]
    print(f"  âœ“ Created {len(all_chunks)} chunks for indexing")

    # Index all chunks
    await vector_index.add_nodes(all_chunks, show_progress=True)

    print(f"âœ… Successfully indexed {len(all_chunks)} chunks!")
    return len(all_chunks)


async def setup_rag_system():
    """Set up the RAG system."""
    print("=" * 70)
    print("ðŸš€ RAG Tool API - OpenAPI Integration")
    print("=" * 70)

    # Initialize embeddings
    print("\n1ï¸âƒ£  Initializing embeddings...")
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        api_key=EMBEDDING_API_KEY,
        base_url=EMBEDDING_BASE_URL
    )
    print(f"   âœ“ Embeddings initialized: {EMBEDDING_MODEL}")

    # Connect to Qdrant
    print(f"\n2ï¸âƒ£  Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}...")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    client.get_collections()
    print(f"   âœ“ Connected to Qdrant")

    # Check if collection exists
    print(f"\n3ï¸âƒ£  Checking collection '{COLLECTION_NAME}'...")
    needs_indexing = not collection_exists(client, COLLECTION_NAME)

    if needs_indexing:
        print(f"   âš ï¸  Collection does not exist - will create and index")
    else:
        print(f"   âœ“ Collection already exists")

    # Create vector store
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embeddings=embeddings,
        distance="Cosine",
        enable_hybrid=ENABLE_HYBRID,
        fusion_method=FUSION_METHOD
    )
    print(f"   âœ“ Vector store created")

    # Create vector index
    vector_index = VectorIndex(
        vector_store=vector_store,
        index_id=INDEX_ID
    )

    # Index documents if needed
    if needs_indexing:
        print(f"\n4ï¸âƒ£  Indexing documents...")
        await load_and_index_documents(
            vector_index=vector_index,
            documents_path=DOCUMENTS_PATH,
            chunk_size=CHUNK_SIZE,
            child_sizes=CHILD_SIZES,
            overlap=CHUNK_OVERLAP
        )
    else:
        print(f"\n4ï¸âƒ£  Skipping document indexing (collection already exists)")

    # Create retriever
    print(f"\n5ï¸âƒ£  Creating retriever...")
    retriever = vector_index.as_retriever(top_k=3, resolve_parents=True)
    print(f"   âœ“ Retriever created")

    # Create agent
    print(f"\n6ï¸âƒ£  Creating RAG agent...")
    retriever_tool = RetrieverTool.from_retriever(retriever)
    tool_func = retriever_tool.get_tool_function()
    tools = [Tool(tool_func, takes_ctx=True, max_retries=3)]

    agent = PydanticAgent.create(
        model=LLM_MODEL,
        tools=tools,
        retries=3
    )
    print(f"   âœ“ Agent created with model: {LLM_MODEL}")

    print("\n" + "=" * 70)
    print("âœ… RAG System Ready!")
    print("=" * 70)

    return agent, vector_index


# ============================================================================
# FastAPI Lifespan
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize the RAG system on startup."""
    try:
        print("\nðŸš€ Starting RAG Tool API Server...")
        agent, vector_index = await setup_rag_system()
        app_state.agent = agent
        app_state.vector_index = vector_index
        app_state.initialized = True
        print(f"\nâœ… Server ready at http://{HOST}:{PORT}")
        print(f"   API docs: http://{HOST}:{PORT}/docs")
        print(f"   OpenAPI schema: http://{HOST}:{PORT}/openapi.json")
        print("=" * 70 + "\n")
        yield
    except Exception as e:
        print(f"\nâŒ Startup Error: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print("\nðŸ‘‹ Shutting down server...")


# ============================================================================
# FastAPI App
# ============================================================================

app = FastAPI(
    title="RAG Tool API",
    description=(
        "This api can be used to search for information."
    ),
    version="1.0.0",
    lifespan=lifespan,
    contact={
        "name": "RAG Tool API",
        "url": "https://github.com/yourusername/fetchcraft",
    },
    license_info={
        "name": "MIT",
    },
)

# Add CORS middleware to handle OPTIONS requests and cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (configure for production)
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods including OPTIONS
    allow_headers=["*"],  # Allow all headers
)


# ============================================================================
# API Endpoints
# ============================================================================

@app.get("/", tags=["General"])
async def root():
    """
    Root endpoint with API information.

    Returns basic information about the API and available endpoints.
    """
    return {
        "name": "RAG Tool API",
        "version": "1.0.0",
        "description": "Search for information.",
        "endpoints": {
            "query": "/query",
            "health": "/health",
            "tool_definition": "/tool-definition",
            "docs": "/docs",
            "openapi": "/openapi.json"
        }
    }


@app.get("/health", response_model=HealthResponse, tags=["General"])
async def health():
    """
    Health check endpoint.

    Returns the current status of the service and configuration information.
    """
    return HealthResponse(
        status="healthy" if app_state.initialized else "initializing",
        version="1.0.0",
        model=LLM_MODEL,
        collection=COLLECTION_NAME,
        hybrid_search_enabled=ENABLE_HYBRID
    )


@app.post("/query", response_model=QueryResponse, tags=["RAG"])
async def query(request: QueryRequest):
    """
    Query the RAG agent with a question.

    This endpoint retrieves relevant documents from the vector database and uses
    an LLM to generate a comprehensive answer based on the retrieved context.

    Args:
        request: Query request containing the question and optional parameters

    Returns:
        QueryResponse with the answer and optional citations

    Raises:
        HTTPException: If the service is not initialized or an error occurs
    """
    if not app_state.initialized or not app_state.agent:
        raise HTTPException(
            status_code=503,
            detail="RAG system not initialized. Please try again in a moment."
        )

    start_time = time.time()

    try:
        # Query the agent
        response = await app_state.agent.query(request.question)

        # Extract answer
        answer = response.response.content

        # Extract citations if requested
        citations = None
        if request.include_citations and response.citations:
            citations = []
            for citation in response.citations[:request.top_k]:
                citations.append(Citation(
                    source=citation.node.metadata.get("source", "Unknown"),
                    filename=citation.node.metadata.get("filename", "N/A"),
                    score=float(citation.node.score) if hasattr(citation.node, "score") else 0.0,
                    text_preview=(
                        citation.node.text[:200] + "..."
                        if len(citation.node.text) > 200
                        else citation.node.text
                    )
                ))

        processing_time = (time.time() - start_time) * 1000  # Convert to ms

        html_output = render_response(
            heading="Answer generated by Wingman ðŸ¤–",  # <-- new optional heading
            answer=answer,
            processing_time=processing_time,
            llm_model=LLM_MODEL,
            citations=citations,
            answer_is_html=False,
            files_base_url="http://wingman.akhbar.home/files",
        )

        return HTMLResponse(
            content=html_output,
            headers={"Content-Disposition": "inline"}
        )

        # return QueryResponse(
        #     answer=answer,
        #     citations=citations,
        #     processing_time_ms=round(processing_time, 2),
        #     model=LLM_MODEL
        # )

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Error processing query: {str(e)}"
        )


@app.get("/tool-definition", response_model=ToolDefinition, tags=["Integration"])
async def get_tool_definition():
    """
    Get the tool definition for integration with AI agents.
    
    This endpoint returns a tool definition in OpenAI function calling format
    that can be used to integrate this RAG service as a tool in other AI agents.
    
    Returns:
        ToolDefinition with function schema for integration
    """
    return ToolDefinition(
        type="function",
        function={
            "name": "query_rag_system",
            "description": (
                "Query a RAG (Retrieval-Augmented Generation) system to answer questions "
                "based on a document collection. This tool retrieves relevant documents "
                "and generates comprehensive answers with source citations."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "The question to ask the RAG system"
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "Number of documents to retrieve (1-10)",
                        "default": 3,
                        "minimum": 1,
                        "maximum": 10
                    },
                    "include_citations": {
                        "type": "boolean",
                        "description": "Whether to include source citations",
                        "default": True
                    }
                },
                "required": ["question"]
            },
            "returns": {
                "type": "object",
                "properties": {
                    "answer": {
                        "type": "string",
                        "description": "The generated answer"
                    },
                    "citations": {
                        "type": "array",
                        "description": "List of source citations",
                        "items": {
                            "type": "object",
                            "properties": {
                                "source": {"type": "string"},
                                "filename": {"type": "string"},
                                "score": {"type": "number"},
                                "text_preview": {"type": "string"}
                            }
                        }
                    }
                }
            }
        }
    )


@app.get("/openapi-tool-spec", tags=["Integration"])
async def get_openapi_tool_spec():
    """
    Get a simplified OpenAPI tool specification for easy integration.
    
    This returns a compact specification that includes:
    - Base URL for the service
    - Authentication requirements
    - Main endpoints and their schemas
    
    This format is designed to be easily consumed by AI agents that support
    OpenAPI tool integration (like GPT Actions, Claude Tools, etc.)
    """
    base_url = f"http://{HOST}:{PORT}"

    return {
        "openapi": "3.1.0",
        "info": {
            "title": "RAG Tool API",
            "description": "Query a document collection using RAG",
            "version": "1.0.0"
        },
        "servers": [
            {
                "url": base_url,
                "description": "RAG Tool API Server"
            }
        ],
        "paths": {
            "/query": {
                "post": {
                    "operationId": "queryRagSystem",
                    "summary": "Query the RAG system",
                    "description": "Retrieve relevant documents and generate an answer",
                    "requestBody": {
                        "required": True,
                        "content": {
                            "application/json": {
                                "schema": {
                                    "type": "object",
                                    "required": ["question"],
                                    "properties": {
                                        "question": {
                                            "type": "string",
                                            "description": "The question to ask"
                                        },
                                        "top_k": {
                                            "type": "integer",
                                            "default": 3,
                                            "minimum": 1,
                                            "maximum": 10
                                        },
                                        "include_citations": {
                                            "type": "boolean",
                                            "default": True
                                        }
                                    }
                                }
                            }
                        }
                    },
                    "responses": {
                        "200": {
                            "description": "Successful response",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "type": "object",
                                        "properties": {
                                            "answer": {"type": "string"},
                                            "citations": {"type": "array"},
                                            "processing_time_ms": {"type": "number"},
                                            "model": {"type": "string"}
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """Run the FastAPI server."""
    import uvicorn

    print("\n" + "=" * 70)
    print("ðŸš€ Starting RAG Tool API Server")
    print("=" * 70)
    print(f"\nConfiguration:")
    print(f"  â€¢ Host: {HOST}")
    print(f"  â€¢ Port: {PORT}")
    print(f"  â€¢ Model: {LLM_MODEL}")
    print(f"  â€¢ Collection: {COLLECTION_NAME}")
    print(f"  â€¢ Hybrid Search: {ENABLE_HYBRID}")
    print("=" * 70 + "\n")

    uvicorn.run(
        "demo.openapi_html_server.openapi_server:app",
        host=HOST,
        port=PORT,
        reload=True,
        log_level="info"
    )


if __name__ == "__main__":
    main()
